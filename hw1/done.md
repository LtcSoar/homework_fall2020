#### ant

效果不错，reward可以与expert policy媲美。

#### Humanoid

效果很差。暂时来看，增加训练量没有显著作用。但我没有时间去验证各种学习参数，以及增加训练量的作用了，理论上来说用Dagger没什么是不行的，只要做的多，distribution总会一样，无非是游戏环境复杂度的差异。

——注意，上面那段话是我在一个错误的“答案”的引导下做出的。应该诚实地说，我参考了cs285_homework_fall2020的结果。但这位同学犯了以下几个错误：1、对于连续动作空间情形，实际上没有采取高斯近似的方法，而是只用了mean net的结果（也就是说输出的是deterministic policy），在ant情形下甚至比采用正经的高斯近似效果还要好，但这是因为ant情形比较简单，对状态空间探索的需求低，确定性策略易于学习；2、对于离散动作空间情形，gym里的定义是这样的，discrete(3)意味着3个动作可以选择而已，所以最后logit_na输出的就是一个各个动作的概率，那么我们最终输出的是选择一个动作（可以做argmax）。当然如果这么做的话，我们真要学，按照监督学习的方法，使用cross_entropy loss是一个比较好的方法。

言而总之，实际上Humanoid和ant都是连续情形。单次学习，ant约为30%，Humanoid只有2%（从return的角度）。而使用Dagger后，ant 2次iter直接return达到快100%；Humanoid30个iter后可以达到20%。

所以说，对于连续空间采用分布近似的方法才是合理的。

